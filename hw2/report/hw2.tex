% Save this file as assignment2.tex
% Create plot1.eps, plot2.eps, plot3.eps in R, using function calls like
%    dev.copy2eps(file="plot1.eps")
% latex assignment2
% latex assignment2  (do twice for figure references and contents)
% dvips -o assignment2.ps -Ppdf assignment2.dvi
% gv assignment2.ps  (to show the postscript file)
% ps2pdf assignment2.ps    (to convert to PDF)
% acroread assignment2.pdf (to show the PDF file)
%  or
% xpdf assignment2.pdf

\documentclass{article}
\usepackage{graphicx}
\usepackage{url}
\usepackage{geometry}
\geometry{verbose,letterpaper,lmargin=1in,rmargin=1in,tmargin=1in,bmargin=1in}
\usepackage{float}

\usepackage{color}  % for color in lstset in listings
\usepackage{listings}
\lstset{language=R,showstringspaces=false}
\lstdefinelanguage{RPlus}[]{R}{%
morekeywords={acf,ar,arima,arima.sim,colMeans,colSums,is.na,is.null,%
mapply,ms,na.rm,nlmin,replicate,row.names,rowMeans,rowSums,seasonal,%
sys.time,system.time,ts.plot,which.max,which.min,solve},%
deletekeywords={c},%
otherkeywords={\%*\%,<-},%
alsoletter={.\%},%
alsoother={:_\$}} 

% Alter some LaTeX defaults for better treatment of figures:
    % See p.105 of "TeX Unbound" for suggested values.
    % See pp. 199-200 of Lamport's "LaTeX" book for details.
    %   General parameters, for ALL pages:
    \renewcommand{\topfraction}{0.9}	% max fraction of floats at top
    \renewcommand{\bottomfraction}{0.8}	% max fraction of floats at bottom
    %   Parameters for TEXT pages (not float pages):
    \setcounter{topnumber}{2}
    \setcounter{bottomnumber}{2}
    \setcounter{totalnumber}{4}     % 2 may work better
    \setcounter{dbltopnumber}{2}    % for 2-column pages
    \renewcommand{\dbltopfraction}{0.9}	% fit big float above 2-col. text
    \renewcommand{\textfraction}{0.07}	% allow minimal text w. figs
    %   Parameters for FLOAT pages (not text pages):
    \renewcommand{\floatpagefraction}{0.7}	% require fuller float pages
	% N.B.: floatpagefraction MUST be less than topfraction !!
    \renewcommand{\dblfloatpagefraction}{0.7}	% require fuller float pages

	% remember to use [htp] or [htpb] for placement


\begin{document}

\title{ICS 663: Homework 2 - Kernel Density Estimation}
\author{Christopher Mullins}
\maketitle

\noindent\hrulefill
\vspace{-5mm} %to remove some whitespace before "Contents"
\tableofcontents
\noindent\hrulefill

\section{Introduction}

Kernel density estimation is a technique for deriving a non-parametric estimate
for the probability density function (PDF) of a dataset. This means it does not
have any assumptions about the underlying distribution of the data it is applied
to.  It works by considering unit-volume regions within the sample space
centered around each point in the training data. 

The estimate, $\hat{p}(\mathbf{x})$, considers the distance of each point in the
training sample from $\mathbf{x}$ and contributes some value to a mean. The
actual calculation is:
\[
	\hat{p}(\mathbf{x}) = \frac{1}{nV} \sum_{i}^{n}
\varphi\left(\frac{\mathbf{x}-\mathbf{x}_i}{h}\right). \]
where $n$ is the number of elements in the training set, $\{ \mathbf{x}_i \}$ is
the training set, $\varphi(\mathbf{u})$ is a {\it kernel function} that
gives some indication how likely it is that $\mathbf{x}$ is in the dataset
$\hat{p}(\mathbf{x})$ describes for each $\mathbf{x}_i$ in the training set.
$V$ is the volume of the region generated by $\varphi(\mathbf{x})$. The most
naive choice for $\varphi$ yields $1$ when $\mathbf{x}$ is in the hypercube of
length $h$ centered at $\mathbf{x}_i$ and $0$ otherwise. Mathematically, this
is:
\[
	\varphi(\mathbf{u}) = \left\{
		\begin{array}{lr}
			1 & : \left| u_i \right| < \frac{1}{2} \qquad \forall 1 \leq i \leq n \\
			0 & \mbox{ otherwise }
		\end{array}
	\right\}.
\]
Notice that $\varphi\left(\frac{\mathbf{x}-\mathbf{x}_i}{h}\right)=1$ if $x$ is
within the hypercube of length $h$ centered at $\mathbf{x}_i$, and $0$
otherwise. Here, $V=h^k$. 

This is very rarely a good choice for $\varphi$, however. Its discontinuousness
can cause problems, and it's probably not often reasonable to consider a point
further away as good of a match as a point that's close. A common alternative is
a 0-mean gaussian:
\[
	\varphi(\mathbf{u}) = \frac{1}{(2\pi)^{k/2}|\Sigma|^{1/2}} 
		\exp\left( -\frac{1}{2}\left(\mathbf{x}^{T} \Sigma^{-1}
		\mathbf{x}\right) \right).
\]
With this choice of $\varphi$, it is hard to calculate the volume. In my
submission, I use an estimate of $h^k$. $k$ is the number of dimensions in
$\mathbf{x}$, and $\Sigma$ is $k \times k$ matrix that is (potentially) related
to the variance of the training data.  Obvious choices for $\Sigma$ are:
\begin{enumerate}
	\item $I_k$ (i.e., unit-variance)
	\item $\sigma^2 I_k$, the mean of the variances for each dimension of data
	\item \label{Sigma-diag}
	$\mbox{diag}\left( \sigma_1^2, \sigma_2^2, \dots, \sigma_n^2 \right)$,
	a diagonal matrix ${a_{ij}}$ where $a_{ii}=\sigma_i^2$ is the variance of
	the $i$th dimension.
	\item $\mbox{cov}\left( \{x_i\} \right)$
\end{enumerate}
In this assignment, multivariate gaussian with $\Sigma$ following case number
\ref{Sigma-diag} is used.

\section{Implementation}

My implementation is again in R. There are three source files: \verb|common.R|,
\verb|parzen_window_estimation.R|, and \verb|hw2.R|. \verb|common.R| includes
some utility methods that I foresee using beyond this assignmenti (for example,
a method that creates a gaussian pdf in an arbitrary number of dimensions).
\verb|parzen_window_estimation.R| includes methods for creating pdf estimations
for a provided dataset. \verb|hw2.R| is a script that seamlessly runs all of
the requested tasks for this assignment.

\subsection{Running}

It should be as simple as running the \verb|hw2.R| script. To run from the
commandline, one could use the command \verb|R --no-save --slave < hw2.R|. The
output should be the table indicating error rates. A \verb|settings| hash in
\verb|hw2.R| control some of the behaviors of this script, including the
kernel function, window widths, etc.

\section{Results}

Results for fixed values of $h$ are shown in table \ref{tab:results1}. The same
measurement is taken 15 times to ensure a smooth result.

\begin{figure}[htb]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
{\bf Run} & {\bf Width = 0.010} & {\bf Width = 0.500} & {\bf Width = 10.000} \\ 
\hline
1 & 0.490196 & 0.078431 & 0.490196\\ 
2 & 0.549020 & 0.058824 & 0.490196\\ 
3 & 0.568627 & 0.000000 & 0.490196\\ 
4 & 0.568627 & 0.039216 & 0.470588\\ 
5 & 0.627451 & 0.000000 & 0.411765\\ 
6 & 0.568627 & 0.078431 & 0.431373\\ 
7 & 0.627451 & 0.039216 & 0.450980\\ 
8 & 0.509804 & 0.078431 & 0.431373\\ 
9 & 0.588235 & 0.039216 & 0.411765\\ 
10 & 0.588235 & 0.058824 & 0.450980\\ 
11 & 0.568627 & 0.000000 & 0.490196\\ 
12 & 0.588235 & 0.039216 & 0.431373\\ 
13 & 0.607843 & 0.039216 & 0.411765\\ 
14 & 0.588235 & 0.039216 & 0.470588\\ 
15 & 0.588235 & 0.039216 & 0.450980\\ 
\hline 
{\bf Mean} & 0.575163 & 0.041830 & 0.452288\\ 
{\bf Variance}  & 0.001410 & 0.000707 & 0.000904\\ 
\hline 
\end{tabular}
\caption{Results using a 0-mean gaussian pdf for $\varphi$}
\label{tab:results1}
\end{figure}

These results highlight the fact that the choice of $h$ is very important to how
successful the kernel density estimation technique is. If a window is too small,
data that are even slightly distant from the training data will have near-0
posterior probability. Too large, and there will be hardly any difference
between points that are very far apart.

\end{document}
